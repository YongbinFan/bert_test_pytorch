{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44c77b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.13 (default, Mar 28 2022, 08:03:21) [MSC v.1916 64 bit (AMD64)]\n",
      "C:\\Users\\35391\\anaconda3\\envs\\pytorch-gpu\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1faf9eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d29f625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2042741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8786687",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"i like eating apples very much, but I don't like pinklady\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5e91eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_mitre = \"I me mine I\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a496b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedder = BertModel.from_pretrained(\"bert-base-cased\", output_hidden_states=True)\n",
    "#         self.embedder = BertModel.from_pretrained(\"bert-base-cased\")\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        tokens = self.tokenizer.tokenize(inputs)\n",
    "        print(tokens)\n",
    "        tokens_id = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "        print(tokens_id)\n",
    "        tokens_id_tensor = torch.tensor(tokens_id).unsqueeze(0)\n",
    "        print(tokens_id_tensor)\n",
    "        outputs = self.embedder(tokens_id_tensor)\n",
    "        print(outputs)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59d3e0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "63b6240b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'like', 'eating', 'apples', 'very', 'much', ',', 'but', 'I', 'don', \"'\", 't', 'like', 'pink', '##lad', '##y']\n",
      "[178, 1176, 5497, 22888, 1304, 1277, 117, 1133, 146, 1274, 112, 189, 1176, 5325, 23850, 1183]\n",
      "tensor([[  178,  1176,  5497, 22888,  1304,  1277,   117,  1133,   146,  1274,\n",
      "           112,   189,  1176,  5325, 23850,  1183]])\n",
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.1420, -0.1883,  0.7285,  ..., -0.0452,  0.3617,  0.8497],\n",
      "         [ 0.3925,  0.0266,  0.3524,  ...,  0.1129, -0.2137,  0.1772],\n",
      "         [-0.1477,  0.0131,  0.3231,  ...,  0.0368, -0.3122,  0.0601],\n",
      "         ...,\n",
      "         [-0.3067, -0.3543,  0.5019,  ..., -0.0569, -0.0640,  0.2124],\n",
      "         [ 0.0754, -0.3582,  0.6924,  ..., -0.4486,  0.1503, -0.1118],\n",
      "         [-0.3347,  0.0661,  0.3251,  ..., -0.1103,  0.0159, -0.3690]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-4.8309e-01,  3.2776e-01,  9.9942e-01, -9.8947e-01,  9.1465e-01,\n",
      "          9.0165e-01,  9.7594e-01, -9.0711e-01, -9.8648e-01, -4.0755e-01,\n",
      "          9.9359e-01,  9.9944e-01, -9.9446e-01, -9.9894e-01,  8.2529e-01,\n",
      "         -9.7584e-01,  9.9085e-01, -3.7881e-01, -9.9990e-01, -1.1350e-01,\n",
      "         -2.4656e-01, -9.9945e-01,  1.2562e-01,  9.8921e-01,  9.3635e-01,\n",
      "          1.1582e-01,  9.9279e-01,  9.9990e-01,  4.7518e-01,  8.0374e-01,\n",
      "          1.2065e-01, -9.9562e-01,  8.6452e-01, -9.9908e-01,  3.4022e-01,\n",
      "         -6.2366e-03,  5.9699e-01, -3.1482e-01,  7.3934e-01, -9.8432e-01,\n",
      "         -5.8463e-01, -8.0130e-02, -3.5734e-01, -3.9737e-01,  8.7156e-01,\n",
      "          1.7062e-01,  5.1068e-01,  7.2264e-03, -1.9644e-01,  9.9660e-01,\n",
      "         -9.5490e-01,  9.9687e-01, -9.6821e-01,  9.9428e-01,  9.9914e-01,\n",
      "          6.4465e-01,  9.9890e-01,  2.1440e-01, -9.9525e-01,  4.5132e-01,\n",
      "          9.5798e-01, -2.2528e-02,  9.3410e-01, -4.3851e-01, -8.4558e-01,\n",
      "         -3.8302e-01, -5.6788e-01,  1.6144e-01, -5.7780e-01,  4.6677e-01,\n",
      "          2.9365e-01,  1.2308e-01,  9.9421e-01, -9.4964e-01, -1.8276e-01,\n",
      "         -9.1957e-01,  5.9183e-01, -9.9982e-01,  9.5471e-01,  9.9985e-01,\n",
      "          2.6642e-01, -9.9973e-01,  9.9887e-01, -9.5750e-02, -1.4167e-01,\n",
      "         -3.5758e-01, -9.9180e-01, -9.9961e-01,  1.9011e-01, -1.3499e-01,\n",
      "          7.3258e-01, -9.9770e-01,  3.5368e-01, -9.4857e-01,  9.9987e-01,\n",
      "         -8.4614e-01, -2.4954e-01,  8.3284e-02,  9.6399e-01, -2.4750e-01,\n",
      "         -7.6648e-01,  8.6244e-01,  9.8995e-01, -9.7413e-01,  9.9757e-01,\n",
      "         -1.6820e-01, -9.0302e-01, -9.1500e-01,  2.7336e-01,  1.4029e-01,\n",
      "          9.8730e-01, -9.2698e-01, -8.6505e-01,  2.4616e-01,  9.7675e-01,\n",
      "         -7.9244e-01,  9.9406e-01,  6.8435e-01, -1.5564e-01,  9.9993e-01,\n",
      "         -1.6220e-01,  9.8653e-01,  9.9911e-01,  8.9595e-01, -6.5213e-01,\n",
      "         -2.9672e-01,  4.5631e-01,  8.9949e-01,  3.4523e-02, -5.1333e-01,\n",
      "          8.4398e-01, -9.9783e-01, -9.9283e-01,  9.9955e-01, -1.5921e-01,\n",
      "          9.9988e-01, -9.9971e-01,  9.9249e-01, -9.9985e-01, -7.1554e-01,\n",
      "         -2.3139e-01,  1.2791e-01, -9.8778e-01,  7.6822e-01,  9.9358e-01,\n",
      "          1.2986e-01, -9.7559e-01, -2.4824e-01,  4.1326e-01, -8.9528e-01,\n",
      "          5.2184e-01,  9.2905e-01, -8.5850e-01,  9.8823e-01,  9.8254e-01,\n",
      "          9.8280e-01,  9.8936e-01, -1.2685e-01, -9.8447e-01,  6.7674e-01,\n",
      "          9.9479e-01, -9.9967e-01,  6.7278e-01, -9.8353e-01,  9.9819e-01,\n",
      "          9.8641e-01,  6.5942e-01, -9.8657e-01,  9.9972e-01, -3.0670e-01,\n",
      "          3.5811e-01, -7.6202e-01, -2.2739e-01, -9.8939e-01,  3.7097e-01,\n",
      "          2.5474e-01,  5.3279e-01,  9.9592e-01, -9.9517e-01,  9.8611e-01,\n",
      "          8.6396e-01, -2.5646e-01,  9.0454e-01,  9.9385e-01, -9.9891e-01,\n",
      "         -9.9378e-01, -9.9725e-01,  7.1680e-01,  3.7061e-01,  3.3745e-01,\n",
      "          2.0837e-01,  9.2095e-01,  9.9007e-01,  2.2082e-01, -9.8300e-01,\n",
      "         -3.5329e-01,  9.9082e-01, -2.0359e-01,  9.9991e-01,  1.6337e-01,\n",
      "         -9.9966e-01, -8.8111e-01,  9.6078e-01,  9.8736e-01, -3.5395e-01,\n",
      "          9.8092e-01, -6.6587e-01,  6.9031e-02,  9.7985e-01, -9.9138e-01,\n",
      "          9.8315e-01, -8.3498e-02,  7.3856e-01,  5.5003e-01,  9.9606e-01,\n",
      "         -7.5273e-01, -1.5050e-01,  2.1409e-01, -3.7491e-01,  9.9990e-01,\n",
      "         -9.9942e-01, -2.9975e-01,  6.5860e-01, -9.9830e-01, -9.9944e-01,\n",
      "          9.8182e-01, -1.1315e-01, -4.5711e-01, -3.7227e-04, -1.3905e-01,\n",
      "          2.3492e-01,  6.8666e-01,  9.9840e-01, -3.8349e-01,  8.4198e-02,\n",
      "         -9.9985e-01, -9.5741e-01, -4.9497e-01, -9.7159e-01,  3.7430e-02,\n",
      "          6.0845e-01, -2.2097e-01, -9.4558e-01, -9.9440e-01,  9.4694e-01,\n",
      "         -2.9498e-01, -8.6246e-01,  2.1819e-01,  5.0559e-01, -9.9040e-01,\n",
      "          4.3991e-01, -9.0409e-01, -9.9312e-01,  9.9953e-01, -1.7290e-01,\n",
      "          9.9377e-01,  9.9239e-01, -9.9770e-01,  7.1718e-01, -9.9314e-01,\n",
      "          8.2391e-02, -9.9738e-01,  7.3127e-02,  1.6174e-01, -6.2123e-01,\n",
      "         -4.8952e-02,  9.9826e-01, -9.1942e-01, -9.3048e-01,  8.5300e-01,\n",
      "         -9.9976e-01,  9.2711e-01, -3.1026e-01,  9.9945e-01,  5.9064e-01,\n",
      "          7.3952e-01,  9.9788e-01,  9.5843e-01, -9.9643e-01, -9.9978e-01,\n",
      "          8.8945e-01,  8.9806e-01, -9.9819e-01, -1.5239e-01,  9.9985e-01,\n",
      "         -9.8697e-01, -5.7719e-01, -9.3881e-01, -9.9816e-01, -9.9946e-01,\n",
      "         -4.1203e-02, -5.2273e-01,  1.0620e-01,  9.7521e-01, -2.1044e-01,\n",
      "         -1.0028e-01,  9.9869e-01,  9.9826e-01,  1.9436e-01,  4.3434e-01,\n",
      "          3.6009e-02, -9.6923e-01, -9.7970e-01, -1.2634e-01,  1.7560e-01,\n",
      "         -9.9989e-01,  9.9946e-01, -9.9856e-01,  9.9545e-01,  9.0108e-01,\n",
      "         -9.7744e-01,  8.2998e-01, -1.4440e-01, -9.2962e-01,  1.9018e-02,\n",
      "          9.9987e-01,  9.7842e-01, -1.8847e-01,  1.3186e-01,  7.9835e-01,\n",
      "         -7.1850e-03,  1.7204e-01, -5.5115e-01, -1.2440e-01,  2.9664e-01,\n",
      "         -9.2382e-01,  9.9026e-01,  8.3760e-01, -9.9853e-01,  9.9140e-01,\n",
      "          6.1344e-02,  6.5287e-01, -7.4322e-01,  6.6391e-01,  9.8758e-01,\n",
      "         -2.0541e-01, -2.3251e-01, -1.0542e-01, -3.3934e-01, -9.6611e-01,\n",
      "          1.6583e-01, -9.9343e-01,  1.4029e-01,  9.8602e-01,  9.9330e-01,\n",
      "         -9.9376e-01,  9.9502e-01, -2.2685e-01,  9.6467e-01, -9.8748e-01,\n",
      "          9.9994e-01, -9.9859e-01,  1.7513e-01,  4.7586e-01, -6.9677e-01,\n",
      "         -9.3090e-02,  9.9678e-01,  9.2888e-01,  9.7407e-01, -9.4511e-01,\n",
      "         -8.2027e-01,  6.6775e-01,  9.9675e-01, -9.8063e-01,  2.1768e-01,\n",
      "         -9.9057e-01, -5.6566e-02,  9.8487e-01,  9.6838e-01, -2.1662e-01,\n",
      "         -8.9362e-01, -9.9278e-01,  8.7891e-01, -9.4300e-01, -5.8226e-01,\n",
      "         -1.0855e-01, -7.0184e-01,  5.3989e-01,  9.9521e-01, -2.1300e-01,\n",
      "          7.5704e-01, -1.3044e-02, -9.9691e-01,  8.4476e-01,  7.8004e-01,\n",
      "          9.9979e-01, -9.9501e-01,  1.7722e-01,  9.9619e-01, -4.0183e-01,\n",
      "         -2.4546e-01,  6.4444e-01,  9.9668e-01, -9.9305e-01, -2.3727e-01,\n",
      "         -9.9972e-01,  2.1075e-01, -7.8486e-01,  4.8658e-01,  4.5319e-01,\n",
      "          2.1242e-01, -6.2839e-01,  9.1235e-01, -9.4112e-02,  6.6474e-01,\n",
      "         -6.1106e-02,  9.2495e-01,  1.8856e-01,  2.3560e-02, -4.6925e-01,\n",
      "          3.8067e-01,  4.1535e-01, -2.3892e-01,  9.8669e-01, -9.2428e-01,\n",
      "          9.9940e-01, -1.5329e-01, -9.9989e-01, -9.7549e-01, -5.9025e-01,\n",
      "         -9.9872e-01,  2.7237e-01, -9.9822e-01,  9.9534e-01,  9.6900e-01,\n",
      "         -9.9363e-01, -9.9392e-01, -9.9898e-01, -9.9705e-01,  4.1102e-01,\n",
      "          6.4849e-02, -1.8374e-01, -4.1214e-01, -5.8644e-02,  1.0699e-01,\n",
      "         -3.2890e-01, -6.2273e-02, -9.1200e-01, -3.4118e-01, -9.8434e-01,\n",
      "          4.4178e-01, -9.9989e-01, -8.7518e-01,  9.9450e-01, -9.8789e-01,\n",
      "         -9.5097e-01, -9.8209e-01, -1.1985e-01, -9.1081e-01,  5.5574e-01,\n",
      "          9.7770e-01,  3.5092e-01, -2.0817e-01, -9.9359e-01,  9.8948e-01,\n",
      "         -8.8745e-01,  8.2739e-02, -9.3598e-01, -9.9328e-01,  9.9969e-01,\n",
      "          8.8833e-01,  3.5334e-02, -1.0224e-01, -9.9722e-01,  9.8589e-01,\n",
      "         -8.5242e-01, -9.7807e-01, -9.9231e-01,  1.7952e-01, -8.5983e-01,\n",
      "         -9.9981e-01,  8.6445e-02,  9.8596e-01,  9.9574e-01,  9.8614e-01,\n",
      "         -2.4830e-01, -8.6658e-02, -8.9727e-01,  1.0021e-01, -9.9990e-01,\n",
      "          6.6639e-01,  8.9922e-01, -9.8225e-01, -3.2300e-01,  9.9872e-01,\n",
      "          9.9288e-01, -9.7236e-01, -9.9507e-01,  9.0717e-01,  4.3662e-01,\n",
      "          7.3781e-01, -3.5435e-02, -8.3248e-01,  1.7932e-01, -3.3300e-01,\n",
      "         -9.9096e-01, -9.5518e-01,  9.9939e-01, -9.9346e-01,  9.8714e-01,\n",
      "          9.9099e-01,  9.9380e-01,  3.2604e-01,  1.6246e-01, -9.8628e-01,\n",
      "         -9.8733e-01, -7.1552e-01, -1.5383e-01, -9.9978e-01,  9.9898e-01,\n",
      "         -9.9987e-01,  3.7756e-01, -2.7549e-01,  8.8597e-01,  9.9360e-01,\n",
      "         -1.1104e-01, -9.9985e-01, -9.9980e-01, -6.0316e-01, -5.5993e-01,\n",
      "          9.9704e-01,  3.5164e-01,  1.8849e-01,  5.6242e-02, -6.0041e-01,\n",
      "          9.9947e-01, -8.7589e-01, -1.5814e-01, -9.8293e-01,  9.9972e-01,\n",
      "          6.3089e-01, -9.9946e-01,  9.9378e-01, -9.9943e-01,  9.3804e-01,\n",
      "          9.8676e-01,  9.8335e-01,  8.9782e-01, -9.9284e-01,  9.9988e-01,\n",
      "         -9.9949e-01,  9.9567e-01, -9.9993e-01, -9.9108e-01,  9.9973e-01,\n",
      "         -9.9282e-01, -4.1060e-01, -9.9952e-01, -9.9499e-01,  3.4229e-01,\n",
      "          1.0558e-01, -6.8808e-01,  9.9672e-01, -9.9932e-01, -9.9918e-01,\n",
      "          2.1475e-01, -8.6735e-01, -4.8360e-01,  9.8968e-01, -1.1193e-03,\n",
      "          9.9813e-01,  5.1501e-02,  9.1444e-01,  2.7997e-01,  9.9667e-01,\n",
      "          9.9831e-01, -8.7123e-01, -5.6115e-01, -9.9897e-01,  9.7190e-01,\n",
      "         -5.2470e-01,  3.6596e-01,  9.2630e-01, -2.1146e-01, -4.2459e-01,\n",
      "          5.8886e-01, -9.9942e-01,  6.0948e-01, -4.4779e-01,  9.0719e-01,\n",
      "          9.4597e-01,  7.7598e-01,  8.4209e-02, -1.6292e-01,  1.0650e-01,\n",
      "         -9.9846e-01,  4.9946e-01, -9.9908e-01,  9.4667e-01, -9.6577e-01,\n",
      "          2.8325e-02, -4.1107e-01,  7.7732e-01, -8.8216e-01,  9.9932e-01,\n",
      "          9.9936e-01, -9.9720e-01,  1.3485e-01,  9.9575e-01,  1.2825e-01,\n",
      "          9.6424e-01, -9.9771e-01, -1.1391e-01,  9.8478e-01, -3.5947e-01,\n",
      "          9.8937e-01,  5.3096e-01, -9.1096e-02,  9.6919e-01, -9.9886e-01,\n",
      "         -8.3978e-01, -7.0604e-01,  1.9137e-01,  1.8983e-01, -9.9469e-01,\n",
      "          3.1927e-01,  9.4856e-01, -4.9441e-02, -9.9973e-01,  8.5930e-01,\n",
      "         -9.9950e-01, -1.2274e-01,  9.9589e-01,  9.2835e-01,  9.9985e-01,\n",
      "         -6.8398e-01, -1.8222e-01, -6.0420e-02, -9.9974e-01, -9.9377e-01,\n",
      "          2.7596e-01, -3.2465e-01, -8.8787e-01,  9.9125e-01,  7.9828e-02,\n",
      "          9.2124e-01, -9.9844e-01,  3.0969e-01,  9.9234e-01,  3.7564e-01,\n",
      "          5.6644e-01, -4.4388e-01, -7.2762e-01, -9.6874e-01, -3.5372e-01,\n",
      "          2.1136e-01,  6.4634e-01, -9.9147e-01, -7.6374e-01, -5.5761e-01,\n",
      "          9.9981e-01, -9.9931e-01, -7.1275e-01, -9.6150e-01,  3.4433e-02,\n",
      "          9.3139e-01,  2.3895e-01, -2.0871e-01, -7.5724e-01,  9.4769e-01,\n",
      "         -9.8329e-01,  9.9862e-01, -9.9868e-01, -9.9883e-01,  9.9988e-01,\n",
      "          1.6586e-01, -9.7968e-01, -3.5364e-01, -3.6603e-01,  9.9410e-02,\n",
      "         -1.3503e-01,  8.2649e-01, -9.0245e-01, -4.7572e-02, -9.9656e-01,\n",
      "          4.1321e-01, -6.7647e-01, -9.9536e-01, -6.6539e-01, -3.4256e-01,\n",
      "         -9.9346e-01,  9.9899e-01,  9.8823e-01,  9.9984e-01, -9.9983e-01,\n",
      "          8.0414e-01,  1.9175e-01,  9.9940e-01, -1.3196e-01, -7.2084e-01,\n",
      "          9.4218e-01,  9.9962e-01, -5.9976e-01,  4.8807e-01, -5.4766e-02,\n",
      "         -3.1196e-02,  4.5621e-02, -8.1750e-01,  9.9003e-01, -9.1500e-01,\n",
      "          7.9673e-01, -9.9375e-01, -9.9972e-01,  9.9970e-01, -6.0053e-02,\n",
      "          9.9767e-01,  5.4643e-02,  4.7684e-01, -5.5263e-01,  9.8869e-01,\n",
      "         -9.8933e-01, -7.2112e-01, -9.9990e-01,  4.0659e-01, -9.9840e-01,\n",
      "         -9.9845e-01,  2.9277e-01,  9.9866e-01, -9.9942e-01, -9.8893e-01,\n",
      "         -3.6157e-03, -9.9992e-01,  8.4601e-01, -9.9314e-01, -7.7479e-01,\n",
      "         -9.9576e-01,  9.8733e-01, -3.7385e-01, -3.3232e-01,  9.8121e-01,\n",
      "         -9.8910e-01,  9.6745e-01,  9.0784e-01,  5.1396e-01,  3.1330e-01,\n",
      "          3.2862e-01, -6.5630e-01, -9.8615e-01, -9.3914e-01, -9.2565e-01,\n",
      "          8.8249e-01, -9.9792e-01, -6.8262e-01,  9.9831e-01,  9.9599e-01,\n",
      "         -9.9818e-01, -9.9815e-01,  9.8773e-01, -6.5151e-01,  9.9620e-01,\n",
      "         -4.9911e-01, -9.9986e-01, -9.9987e-01, -1.4821e-01, -3.2011e-01,\n",
      "          9.9928e-01, -1.6706e-01,  9.8655e-01,  5.9419e-01,  5.1615e-01,\n",
      "         -5.8004e-01, -5.7587e-01, -1.1419e-01, -2.2691e-01, -2.4792e-01,\n",
      "          9.9990e-01, -5.0147e-01,  9.9866e-01]], grad_fn=<TanhBackward>), hidden_states=(tensor([[[-0.3828, -0.4006,  0.4215,  ..., -0.2551,  0.6025,  0.5101],\n",
      "         [-0.4440, -0.1781,  0.1671,  ..., -0.5643,  0.6802, -0.3683],\n",
      "         [ 0.9004,  0.4391, -0.8908,  ..., -0.0792, -1.8930,  0.7977],\n",
      "         ...,\n",
      "         [-0.4251, -0.4252,  1.1545,  ..., -0.3241,  0.5745, -0.7285],\n",
      "         [ 0.2618,  0.5249,  0.7885,  ...,  0.9924, -0.7514, -0.2856],\n",
      "         [-0.3472,  0.1468, -0.5952,  ..., -0.7847,  0.6977,  0.3277]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.4204, -0.3042,  0.4118,  ..., -0.2424,  0.3855,  0.4510],\n",
      "         [-0.8193, -0.1032, -0.4179,  ..., -0.9609,  1.0501,  0.1664],\n",
      "         [ 0.9122,  0.4425, -1.2631,  ...,  0.2010, -2.2907,  1.1313],\n",
      "         ...,\n",
      "         [-0.9374, -0.3464,  1.1706,  ..., -0.5274,  0.4599, -0.3964],\n",
      "         [-0.0838,  1.0937,  0.6742,  ...,  0.9757, -0.6988,  0.4525],\n",
      "         [-0.4197,  0.0336, -0.4047,  ..., -0.8254,  0.8309,  0.7139]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.1367, -0.3328,  0.5180,  ..., -0.3606,  0.2981,  0.1625],\n",
      "         [-0.1123, -0.0737, -0.6240,  ..., -0.9537,  0.7956, -0.4117],\n",
      "         [ 0.4517,  0.6359, -0.5264,  ..., -0.0216, -1.7256,  1.1525],\n",
      "         ...,\n",
      "         [-1.4891, -0.3554,  1.0834,  ..., -0.1500,  0.4295, -0.1458],\n",
      "         [ 0.3350,  1.3002,  1.3143,  ...,  0.4769, -0.0835,  0.4111],\n",
      "         [-0.8075, -0.3247, -0.0585,  ..., -0.7504,  0.5591,  0.7441]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.0348, -0.0129,  0.5502,  ..., -0.4929,  0.4236,  0.0265],\n",
      "         [-0.1794,  0.0070, -0.5736,  ..., -0.9355,  0.8140, -0.0266],\n",
      "         [ 0.2650,  0.2588, -0.8082,  ..., -0.1453, -1.4116,  1.5057],\n",
      "         ...,\n",
      "         [-1.8737, -0.6876,  1.2994,  ..., -0.0972,  0.6437, -0.2984],\n",
      "         [ 0.6213,  0.9012,  0.6538,  ...,  0.7702,  0.0831,  0.0996],\n",
      "         [-0.5515, -0.5313, -0.1473,  ..., -0.6907,  0.1617,  0.5107]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.1335, -0.5007,  0.6680,  ..., -0.6848,  0.4100, -0.0019],\n",
      "         [-0.1161, -0.1902, -0.3340,  ..., -0.5726,  0.7754, -0.3039],\n",
      "         [ 0.4529, -0.1735, -0.7270,  ...,  0.1782, -1.0043,  1.3633],\n",
      "         ...,\n",
      "         [-1.2814, -0.8867,  1.6482,  ...,  0.2989,  0.4462, -0.8856],\n",
      "         [ 0.8676,  0.3998,  1.4340,  ...,  0.8672,  0.1475, -0.3683],\n",
      "         [-0.7163, -0.4993,  0.0840,  ..., -0.3751, -0.0707, -0.2539]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.2503, -0.6377,  0.4216,  ..., -0.7335,  0.2866, -0.1819],\n",
      "         [-0.0063, -0.2130, -0.6967,  ..., -0.2210,  0.2302,  0.3658],\n",
      "         [ 0.4505,  0.1614, -0.3860,  ..., -0.1687, -0.4395,  1.3539],\n",
      "         ...,\n",
      "         [-1.0287, -0.7619,  1.2737,  ...,  0.5693,  0.5697, -0.2781],\n",
      "         [ 0.5990,  0.7735,  0.3752,  ...,  1.2330,  0.7032, -0.3321],\n",
      "         [-0.9045, -0.0642,  0.1682,  ...,  0.2897, -0.5763, -0.1708]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.3486, -0.3358,  0.5593,  ..., -1.0560, -0.1073,  0.2128],\n",
      "         [ 0.3772,  0.3205, -0.7372,  ...,  0.0211, -0.3229,  0.3898],\n",
      "         [-0.0325,  0.5062, -0.3700,  ...,  0.2075, -0.7335,  1.4323],\n",
      "         ...,\n",
      "         [-1.1463, -0.7299,  1.6128,  ...,  0.2772,  0.4440,  0.6697],\n",
      "         [ 0.2499,  0.8784,  0.4862,  ...,  1.0976, -0.0949,  0.2206],\n",
      "         [-0.9208,  0.1939,  0.3738,  ...,  0.5115, -1.2554,  0.0470]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.1741, -0.4578,  0.1618,  ..., -1.0611, -0.2375,  0.6271],\n",
      "         [ 0.3363, -0.1391, -0.9707,  ..., -0.3859, -0.3864,  0.5637],\n",
      "         [-0.0717, -0.0987, -0.5951,  ..., -0.3460, -0.4667,  1.4254],\n",
      "         ...,\n",
      "         [-0.8328, -1.1206,  1.4621,  ...,  0.2285,  0.2737,  1.1011],\n",
      "         [ 0.7196, -0.0311,  0.9663,  ...,  0.9166, -0.1018,  0.1501],\n",
      "         [-0.9094, -0.3125,  0.5359,  ...,  0.4958, -1.4188, -0.1382]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.0702, -0.2079,  0.3807,  ..., -1.1750, -0.2468,  0.9367],\n",
      "         [-0.1053, -0.4188, -0.6985,  ..., -0.2768, -0.6987,  0.4674],\n",
      "         [-0.4616, -0.0613, -0.7568,  ..., -0.1736, -0.3749,  1.4551],\n",
      "         ...,\n",
      "         [-0.6147, -0.9402,  1.3934,  ...,  0.3191,  0.2182,  0.8801],\n",
      "         [ 0.6958, -0.1262,  1.0326,  ...,  0.5340, -0.0914,  0.0404],\n",
      "         [-0.8387,  0.1844,  0.4466,  ...,  0.4948, -0.9477, -0.2379]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.0936,  0.3672,  0.5305,  ..., -0.8874, -0.1188,  1.3044],\n",
      "         [-0.0840, -0.3309, -0.3493,  ..., -0.0634, -0.4213,  0.5151],\n",
      "         [-0.4672,  0.1087, -0.5898,  ..., -0.2520, -0.1618,  0.9710],\n",
      "         ...,\n",
      "         [-1.3636, -0.8468,  1.2061,  ...,  0.2817,  0.2614,  0.9199],\n",
      "         [-0.4169, -0.2856,  1.3357,  ..., -0.1046,  0.3488,  0.1047],\n",
      "         [-1.7403,  0.1118,  0.6651,  ...,  0.1202, -0.4420, -0.3957]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.0801,  0.3289,  0.2272,  ..., -0.5927,  0.3080,  1.2815],\n",
      "         [ 0.2823, -0.1658, -0.2006,  ...,  0.0633, -0.2124,  0.5113],\n",
      "         [-0.4630,  0.1367, -0.3873,  ..., -0.0187, -0.4822,  0.7639],\n",
      "         ...,\n",
      "         [-1.1827, -0.6492,  1.0508,  ...,  0.0409,  0.0091,  0.6983],\n",
      "         [-0.1957, -0.3676,  1.1397,  ..., -0.2616,  0.2582,  0.2140],\n",
      "         [-1.1983,  0.2913,  0.5395,  ..., -0.2297, -0.1364, -0.2758]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.5703,  0.0828,  0.6581,  ..., -0.2824,  0.2056,  1.0348],\n",
      "         [ 0.1578, -0.1881,  0.6223,  ...,  0.7781, -0.4517, -0.0415],\n",
      "         [-0.5035,  0.1149,  0.2705,  ...,  0.5759, -0.4173,  0.3250],\n",
      "         ...,\n",
      "         [-0.9292, -0.6747,  1.1718,  ...,  0.2968, -0.0719,  0.2991],\n",
      "         [-0.6842, -0.5677,  1.1952,  ..., -0.2322,  0.2448, -0.1612],\n",
      "         [-1.3653,  0.0549,  0.4430,  ..., -0.0380, -0.2088, -0.9422]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.1420, -0.1883,  0.7285,  ..., -0.0452,  0.3617,  0.8497],\n",
      "         [ 0.3925,  0.0266,  0.3524,  ...,  0.1129, -0.2137,  0.1772],\n",
      "         [-0.1477,  0.0131,  0.3231,  ...,  0.0368, -0.3122,  0.0601],\n",
      "         ...,\n",
      "         [-0.3067, -0.3543,  0.5019,  ..., -0.0569, -0.0640,  0.2124],\n",
      "         [ 0.0754, -0.3582,  0.6924,  ..., -0.4486,  0.1503, -0.1118],\n",
      "         [-0.3347,  0.0661,  0.3251,  ..., -0.1103,  0.0159, -0.3690]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)), past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "results = model(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ecc2a013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'me', 'mine', 'I']\n",
      "[146, 1143, 2317, 146]\n",
      "tensor([[ 146, 1143, 2317,  146]])\n",
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.4292,  0.1015, -0.0219,  ..., -0.0104, -0.1396,  0.5375],\n",
      "         [-0.2785, -0.0881, -0.3081,  ...,  0.2058, -0.1023,  0.5308],\n",
      "         [-0.5109,  0.2165, -0.3667,  ...,  0.3497, -0.1653,  0.5240],\n",
      "         [-0.2754, -0.0215, -0.1724,  ...,  0.3528, -0.3726,  0.3188]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), pooler_output=tensor([[-0.5348,  0.2797,  0.9939, -0.9611,  0.7495,  0.1304,  0.9471, -0.4200,\n",
      "         -0.9063, -0.7057,  0.9329,  0.9879, -0.8991, -0.9893, -0.0255, -0.9297,\n",
      "          0.9451, -0.2202, -0.9987, -0.4641,  0.5326, -0.9922,  0.3263,  0.8361,\n",
      "          0.8769,  0.1434,  0.9671,  0.9982,  0.7515,  0.6816,  0.2141, -0.9670,\n",
      "         -0.4144, -0.9870,  0.2175, -0.4053, -0.5819, -0.2018, -0.3722, -0.4772,\n",
      "         -0.7118, -0.4716, -0.5800, -0.3315,  0.5304,  0.6075,  0.5060, -0.1721,\n",
      "         -0.3220,  0.9949, -0.9220,  0.9995, -0.7261,  0.9963,  0.9904,  0.5689,\n",
      "          0.9824,  0.0836, -0.8223,  0.7071,  0.8870, -0.1015,  0.9484, -0.5292,\n",
      "         -0.7608, -0.6107,  0.1884, -0.0605, -0.7619,  0.4566,  0.6097,  0.0505,\n",
      "          0.9835, -0.8065, -0.0875, -0.9415,  0.7103, -0.9976,  0.9303,  0.9965,\n",
      "         -0.5642, -0.9937,  0.9885, -0.0623,  0.3715, -0.8541, -0.8300, -0.9948,\n",
      "          0.1176, -0.5731,  0.3903, -0.9726, -0.3032, -0.2928,  0.9983, -0.5948,\n",
      "          0.0542,  0.1690,  0.5503, -0.0309, -0.6058,  0.5870,  0.7537, -0.6704,\n",
      "          0.9360, -0.4361, -0.8190, -0.6824, -0.6067,  0.0414,  0.9189, -0.9255,\n",
      "         -0.4370,  0.1385,  0.3063, -0.1295,  0.9549,  0.1609,  0.0443,  0.9986,\n",
      "         -0.1713,  0.8448,  0.9877,  0.2517, -0.0788, -0.2600,  0.3563,  0.7156,\n",
      "          0.5029, -0.4998,  0.6548, -0.9834, -0.7714,  0.9916, -0.2553,  0.9982,\n",
      "         -0.9945,  0.7637, -0.9977, -0.3284,  0.2464,  0.0562, -0.7989,  0.8407,\n",
      "          0.9699,  0.2091, -0.6536,  0.5119, -0.5059,  0.0342,  0.7057,  0.8536,\n",
      "         -0.8629,  0.9989,  0.8469,  0.5145,  0.7697, -0.0715, -0.8272,  0.4357,\n",
      "          0.9649, -0.9962,  0.5921, -0.7347,  0.9854,  0.9512,  0.4755, -0.7913,\n",
      "          0.9942,  0.5739,  0.4632, -0.6737, -0.4205, -0.8396,  0.2756,  0.0572,\n",
      "          0.5413,  0.9889, -0.9502,  0.9713,  0.9903, -0.3629,  0.8435,  0.8472,\n",
      "         -0.9738, -0.9634, -0.9815,  0.6610, -0.7768, -0.3854, -0.4791,  0.8759,\n",
      "          0.7353,  0.5699, -0.9615, -0.1110,  0.9596, -0.1406,  0.9980,  0.5517,\n",
      "         -0.9926,  0.1492,  0.2284,  0.9398, -0.1770,  0.8960,  0.0322,  0.1432,\n",
      "          0.8547, -0.9992,  0.6862, -0.2232, -0.3476,  0.6817,  0.9743, -0.3862,\n",
      "         -0.1104,  0.1133,  0.2366,  0.9978, -0.9920, -0.1916,  0.8109, -0.9850,\n",
      "         -0.9896,  0.9275, -0.0958,  0.6446, -0.0919, -0.7455,  0.0137, -0.0350,\n",
      "          0.9834,  0.0789,  0.7329, -0.9961, -0.5414, -0.8564, -0.7825, -0.2188,\n",
      "          0.2780, -0.2648, -0.9011, -0.7454,  0.8784,  0.5968, -0.0066, -0.6901,\n",
      "          0.5237, -0.8247, -0.1178, -0.7350, -0.9615,  0.9897,  0.5632,  0.8762,\n",
      "          0.9629, -0.9820,  0.2045, -0.8404,  0.0737, -0.9990, -0.2128, -0.7935,\n",
      "          0.2213, -0.2559,  0.9828, -0.9390, -0.3312,  0.3493, -0.9945,  0.7883,\n",
      "         -0.1724,  0.9935,  0.6975,  0.7641,  0.9834,  0.6761, -0.9821, -0.9964,\n",
      "          0.4688,  0.9900, -0.9794, -0.2044,  0.9968, -0.7122, -0.4509, -0.9079,\n",
      "         -0.9758, -0.9893, -0.0846, -0.2251,  0.0075,  0.8743, -0.6679, -0.1316,\n",
      "          0.9857,  0.9984,  0.0195,  0.5333, -0.1159, -0.9010, -0.9954, -0.3588,\n",
      "          0.0667, -0.9973,  0.9940, -0.9835,  0.9994, -0.0278, -0.6762,  0.8090,\n",
      "         -0.1092, -0.7128, -0.1022,  0.9981,  0.9064, -0.1646,  0.1674, -0.1872,\n",
      "          0.0847, -0.6869,  0.3823,  0.5577,  0.3122, -0.8566,  0.7233,  0.4280,\n",
      "         -0.9798,  0.8908,  0.2646,  0.7493, -0.3057,  0.5619,  0.9266, -0.0353,\n",
      "          0.3931, -0.3487, -0.9222, -0.7483,  0.1583, -0.8615,  0.5302,  0.8147,\n",
      "          0.9500, -0.9214,  0.9959, -0.1916,  0.6924, -0.7855,  0.9985, -0.9937,\n",
      "         -0.0465, -0.2067, -0.0388,  0.7676,  0.9679,  0.5263,  0.8737, -0.9273,\n",
      "         -0.2632,  0.9029,  0.9656, -0.6991, -0.0815, -0.6609,  0.6651,  0.9468,\n",
      "          0.6347, -0.0446, -0.8561, -0.8658,  0.9123, -0.4973, -0.3799, -0.0861,\n",
      "          0.0089, -0.0583,  0.8288,  0.4506,  0.2983, -0.0534, -0.9667,  0.1065,\n",
      "          0.2147,  0.9943, -0.9683, -0.6541,  0.9804, -0.1642,  0.4233,  0.5992,\n",
      "          0.8411, -0.9889, -0.0955, -0.9959, -0.1850, -0.5236,  0.5501,  0.3609,\n",
      "          0.0539,  0.5283,  0.7907, -0.5837,  0.5912, -0.2094,  0.7311,  0.4833,\n",
      "         -0.0719, -0.2803,  0.6016,  0.2002, -0.0096,  0.9662, -0.9251,  0.9964,\n",
      "         -0.1613, -0.9975, -0.4235, -0.4336, -0.9950, -0.4730, -0.9968,  0.9630,\n",
      "          0.4115, -0.8532, -0.8419, -0.9964, -0.9988,  0.0787, -0.4677, -0.1638,\n",
      "         -0.4558,  0.8895,  0.1305,  0.3390,  0.2287, -0.9063, -0.0211, -0.7109,\n",
      "          0.5230, -0.9954,  0.0131,  0.8570, -0.6395, -0.7574, -0.9617,  0.7475,\n",
      "         -0.7796,  0.4510,  0.9392,  0.7270,  0.6126, -0.9873,  0.9626, -0.4126,\n",
      "          0.0126, -0.3332, -0.9289,  0.9952,  0.0214,  0.0591, -0.1680, -0.9852,\n",
      "          0.7280, -0.4028, -0.6971, -0.9423,  0.3903, -0.7780, -0.9973, -0.1238,\n",
      "          0.7643,  0.9946,  0.9343,  0.2147, -0.3603, -0.7747,  0.1711, -0.9986,\n",
      "         -0.1907,  0.2888, -0.9222,  0.2447,  0.9735,  0.9672, -0.7275, -0.8968,\n",
      "         -0.5296,  0.5880,  0.7887,  0.6192, -0.7545,  0.0485, -0.2712, -0.9386,\n",
      "         -0.8952,  0.9897, -0.7937,  0.9644,  0.7779,  0.8807,  0.3179, -0.2733,\n",
      "         -0.6313, -0.9959, -0.7993, -0.2587, -0.9940,  0.9942, -0.9966, -0.4573,\n",
      "          0.6688,  0.0260,  0.9556,  0.4120, -0.9976, -0.9972, -0.8253, -0.6509,\n",
      "          0.9691, -0.0038,  0.1137,  0.1531,  0.0961,  0.9904, -0.3789,  0.5435,\n",
      "         -0.6464,  0.9953,  0.4662, -0.9922,  0.7790, -0.9944,  0.5847,  0.8606,\n",
      "          0.9549,  0.9051, -0.8623,  0.9968, -0.9966,  0.9931, -0.9980, -0.7408,\n",
      "          0.9962, -0.9615, -0.2208, -0.9911, -0.8256, -0.2977,  0.0693, -0.5232,\n",
      "          0.9786, -0.9909, -0.9884,  0.3477, -0.7668,  0.6497,  0.7878,  0.2828,\n",
      "          0.9852,  0.1077,  0.8600, -0.1950,  0.8943,  0.9989, -0.5506, -0.5019,\n",
      "         -0.9849,  0.8367,  0.1522,  0.2753,  0.8045, -0.1369,  0.1770,  0.5361,\n",
      "         -0.9890,  0.1609, -0.8173,  0.6120,  0.4954,  0.4769,  0.1537,  0.2140,\n",
      "          0.0028, -0.9766,  0.2817, -0.9926,  0.9274, -0.4914, -0.0397, -0.2053,\n",
      "          0.8177, -0.8568,  0.9876,  0.9850, -0.9997,  0.2073,  0.9719,  0.6091,\n",
      "          0.8966, -0.9796, -0.0932,  0.8180, -0.4726,  0.9589,  0.6900,  0.0033,\n",
      "          0.7021, -0.9810, -0.5250, -0.7632,  0.0226,  0.4976, -0.9685,  0.2178,\n",
      "          0.7797,  0.2830, -0.9921, -0.0630, -0.9914,  0.0195,  0.9826,  0.6764,\n",
      "          0.9965,  0.1078, -0.1746, -0.2486, -0.9951, -0.8268,  0.2431, -0.0782,\n",
      "          0.3085,  0.8083,  0.5808,  0.1506, -0.9905,  0.2150,  0.7941,  0.0837,\n",
      "          0.9145,  0.3283, -0.8432, -0.7384, -0.6937,  0.1281,  0.3448, -0.9618,\n",
      "          0.1631, -0.3436,  0.9980, -0.9794, -0.5598, -0.8112, -0.6139,  0.3306,\n",
      "          0.2634, -0.0978, -0.0234,  0.7240, -0.8219,  0.9833, -0.9860, -0.9809,\n",
      "          0.9969, -0.6103, -0.7220, -0.5186, -0.4058, -0.4632, -0.0073,  0.3520,\n",
      "         -0.8338,  0.0465, -0.9955, -0.4528, -0.3647, -0.9779, -0.7089, -0.5126,\n",
      "         -0.9996,  0.9811,  0.9531,  0.9975, -0.9962,  0.7655,  0.1589,  0.9925,\n",
      "          0.0131, -0.5278,  0.7947,  0.9961, -0.0201,  0.1152, -0.0755,  0.0822,\n",
      "         -0.3976, -0.7002,  0.7798, -0.7044,  0.7312, -0.9600, -0.9938,  0.9977,\n",
      "         -0.1037,  0.9722,  0.0966, -0.7156, -0.6422,  0.8897, -0.8049, -0.6843,\n",
      "         -0.9976,  0.6890, -0.9995, -0.9862,  0.4970,  0.9713, -0.9927, -0.9409,\n",
      "         -0.0516, -0.9978,  0.4319, -0.8586,  0.2915, -0.9618,  0.6635, -0.4407,\n",
      "          0.5637,  0.9427, -0.9454,  0.5858,  0.3953,  0.9447,  0.0734,  0.2572,\n",
      "         -0.2155, -0.8650, -0.7726, -0.8531,  0.1934, -0.9762, -0.8362,  0.9847,\n",
      "          0.9563, -0.9884, -0.9822,  0.8588, -0.5513,  0.9802, -0.4577, -0.9969,\n",
      "         -0.9974, -0.1601, -0.5475,  0.9896, -0.1980,  0.9979,  0.7478,  0.6071,\n",
      "         -0.6011,  0.0919,  0.2367,  0.3332, -0.1332,  0.9978, -0.0867,  0.9840]],\n",
      "       grad_fn=<TanhBackward>), hidden_states=(tensor([[[-0.1011,  0.2909,  0.3513,  ...,  0.8033, -0.4681,  0.5899],\n",
      "         [-0.6578, -0.0644, -0.7429,  ...,  0.0888,  0.2104,  0.3578],\n",
      "         [ 0.0856, -0.1647, -0.1094,  ...,  0.0071, -0.6350,  0.5536],\n",
      "         [-0.8275,  0.3400,  0.5433,  ...,  1.0405, -0.7707,  0.2448]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.0713,  0.0225,  0.1002,  ...,  0.3299, -0.2830,  0.4216],\n",
      "         [-0.7366, -0.3231, -0.9924,  ..., -0.4782,  0.5235,  0.5124],\n",
      "         [-0.7333,  0.1653, -0.6169,  ...,  0.1370, -0.0421,  0.6953],\n",
      "         [-0.5944, -0.1868,  0.2776,  ...,  1.3676, -0.9306,  0.8728]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.0444,  0.1874, -0.4405,  ...,  0.7067, -0.0922, -0.5332],\n",
      "         [-0.2203, -0.4764, -1.7074,  ..., -0.5709,  1.0297, -0.5954],\n",
      "         [-0.9991,  1.0582, -1.2231,  ...,  0.0284,  0.2131, -0.3501],\n",
      "         [-0.3901, -0.0408, -0.2492,  ...,  1.4352, -0.4223, -0.0032]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.2427,  0.1314, -0.6549,  ...,  0.6605,  0.1783, -0.5830],\n",
      "         [ 0.3018, -0.9228, -1.4325,  ..., -0.4876,  1.0176, -0.6927],\n",
      "         [-0.6372,  0.7732, -1.0878,  ...,  0.2093,  0.6482, -0.9946],\n",
      "         [-0.1365, -0.6460, -0.3062,  ...,  1.4992,  0.1245, -0.1385]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 4.9066e-01, -1.3401e-03, -6.0686e-01,  ...,  7.4831e-01,\n",
      "           3.2423e-02,  1.1632e-01],\n",
      "         [ 4.0140e-01, -6.5356e-01, -1.4394e+00,  ..., -6.4793e-01,\n",
      "           8.9136e-01,  2.5703e-01],\n",
      "         [-8.2033e-01,  9.6159e-01, -9.9762e-01,  ...,  3.7931e-01,\n",
      "           7.5809e-01, -7.1534e-02],\n",
      "         [-1.1818e-01, -5.8238e-01, -2.9949e-01,  ...,  1.0866e+00,\n",
      "           1.0010e-01,  1.2004e+00]]], grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.8358, -0.3939, -0.4695,  ...,  0.4755, -0.1468, -0.1294],\n",
      "         [ 0.4166, -0.8420, -1.2846,  ..., -0.5536,  1.1621,  0.2340],\n",
      "         [-0.6816,  1.0236, -1.4147,  ...,  0.5063,  0.3420,  0.2319],\n",
      "         [ 0.0614, -0.8512, -0.3416,  ...,  0.9171,  0.2026,  0.8925]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.4803, -0.3787, -0.6354,  ..., -0.2074,  0.0171,  0.2297],\n",
      "         [-0.1835, -0.7615, -1.7943,  ..., -0.1578,  0.8086,  0.6081],\n",
      "         [-1.3243,  0.7268, -1.8260,  ...,  0.5993,  0.1334,  0.7976],\n",
      "         [-0.2441, -0.5700, -0.8612,  ...,  1.1218,  0.0788,  0.9860]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.4367, -0.4023, -0.3799,  ..., -0.0900,  0.6035,  0.6351],\n",
      "         [ 0.0512, -0.7005, -1.0798,  ...,  0.3188,  0.8566,  1.0300],\n",
      "         [-0.6776,  0.2543, -1.6141,  ...,  0.8111,  0.3156,  1.0570],\n",
      "         [ 0.1268, -0.8374, -0.8766,  ...,  1.3269,  0.2007,  0.9274]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.2223,  0.1839,  0.0450,  ..., -0.1936,  0.4565,  0.7905],\n",
      "         [ 0.0634, -0.1958, -0.6325,  ...,  0.2397,  0.6439,  0.9016],\n",
      "         [-0.6869,  0.5250, -0.8728,  ...,  0.5812,  0.1242,  0.7245],\n",
      "         [-0.0333, -0.3249, -0.3062,  ...,  0.9179, -0.3231,  0.2272]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[ 0.1543,  0.0943, -0.0747,  ...,  0.2498,  0.6647,  0.9341],\n",
      "         [ 0.2118, -0.2274, -0.6500,  ...,  0.7633,  0.8890,  1.1476],\n",
      "         [-0.3777,  0.4140, -0.7539,  ...,  1.0605,  0.5212,  1.0399],\n",
      "         [ 0.1641, -0.3142, -0.0576,  ...,  1.2963, -0.0728,  0.2323]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.0046,  0.3172, -0.1410,  ...,  0.0509,  0.3926,  1.1563],\n",
      "         [-0.0470, -0.2397, -0.6226,  ...,  0.5289,  0.8914,  1.2445],\n",
      "         [-0.6820,  0.3242, -0.8339,  ...,  0.8239,  0.6148,  1.1170],\n",
      "         [-0.0570, -0.2922, -0.2933,  ...,  0.9770,  0.2857,  0.2682]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.9311, -0.1412, -0.1668,  ..., -0.0510,  0.0919,  0.8355],\n",
      "         [-0.8660, -0.4841, -0.6195,  ...,  0.3444,  0.5687,  0.8051],\n",
      "         [-1.4476, -0.0237, -0.8015,  ...,  0.7196,  0.3378,  0.7057],\n",
      "         [-1.0021, -0.4543, -0.2120,  ...,  0.7146,  0.0250,  0.2232]]],\n",
      "       grad_fn=<NativeLayerNormBackward>), tensor([[[-0.4292,  0.1015, -0.0219,  ..., -0.0104, -0.1396,  0.5375],\n",
      "         [-0.2785, -0.0881, -0.3081,  ...,  0.2058, -0.1023,  0.5308],\n",
      "         [-0.5109,  0.2165, -0.3667,  ...,  0.3497, -0.1653,  0.5240],\n",
      "         [-0.2754, -0.0215, -0.1724,  ...,  0.3528, -0.3726,  0.3188]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)), past_key_values=None, attentions=None, cross_attentions=None)\n"
     ]
    }
   ],
   "source": [
    "results = model(sentence_mitre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2fb644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
